{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep-learning framework that allows you to \n",
    "- build a neural network of aribitrary complexity\n",
    "- perform computations on <b>hardware accelerators</b> (GPUs, TPUs, ...)\n",
    "- automatically compute the gradient of the loss function w.r.t. the weight vectors in the neural network\n",
    "\n",
    "Tensors are at the core of PyTorch, as they are the only way data is being represented in Pytorch.\n",
    "\n",
    "Whether you have texts, images, videos or even molecules as your input data, you will have to convert them into tensors somehow before unleashing the power of PyTorch!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch we use tensors to encode the inputs and outputs of a model, as well as the model's parameters.\n",
    "\n",
    "Tensors are similar to NumPy's ndarrays, except that tensors can run on GPUs or other <b>hardware accelerators</b>. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data. Tensors are also optimized for automatic <b>automatic differentiation</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be created directly from data. The data type is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2], [3,4]]\n",
    "print(data)\n",
    "\n",
    "x_data = torch.tensor(data)\n",
    "x_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From a NumPy array\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "print(np_array)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From another tensor\n",
    "\n",
    "The new tensor retains the properties (shape, data type) of the argument tensor, unless explicitly overridden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Fives Tensor: \n",
      " tensor([[5, 5],\n",
      "        [5, 5]])\n",
      "\n",
      "X dtype: torch.int64\n",
      "Random Tensor: \n",
      " tensor([[0.6179, 0.2519],\n",
      "        [0.2334, 0.1559]]) \n",
      "\n",
      "X_rand dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_fives = torch.full_like(x_data, 5)\n",
    "print(f\"Fives Tensor: \\n {x_fives}\\n\")\n",
    "\n",
    "# Original x_data inside tensors were of int type. Now it is float\n",
    "print(f\"X dtype: {x_data.dtype}\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "print(f\"X_rand dtype: {x_rand.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With random or constant values\n",
    "\n",
    "<i>shape</i> is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor. \n",
    "\n",
    "Another commonly used one is torch.randn that draws numbers from the standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.4174, 0.6166, 0.2953],\n",
      "        [0.4896, 0.0758, 0.3680]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n",
      "Fives Tensor: \n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will not use \"_like\" since we are not copying another tensor's shape, \n",
    "# but we are initializing a new tensor while initializing its shape\n",
    "\n",
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "fives_tensor = torch.full(shape, fill_value=5.0)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")\n",
    "print(f\"Fives Tensor: \\n {fives_tensor} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of a Tensor\n",
    "\n",
    "Tensor attributes describe their shape, data type, and the device on which they are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: 3\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor also has a <b>method</b> called <i>size</i> to return its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because <i>size</i> is a <b>method</b>, you can actually pass in an argument specifying a single dimension that you want to know the size of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dim 0: 3\n",
      "Size of dim 1: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of dim 0: {tensor.size(dim=0)}\") # \"dim\" is whats known as an \"axis\" in NumPy\n",
    "print(f\"Size of dim 1: {tensor.size(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just slice the shape attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor 0: 3\n",
      "Shape of tensor 1: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of tensor 0: {tensor.shape[0]}\")\n",
    "print(f\"Shape of tensor 1: {tensor.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GPUs\n",
    "\n",
    "How can we know if a GPU is available in our machine or not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Tensors\n",
    "\n",
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing)\n",
    "\n",
    "Each of these operations can be run on GPU (at typically higher speed than on a CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a boolean. True, if CUDA is set up and there is a GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"GPU\"s are often referred to as \"cuda\" in PyTorch. \n",
    "\n",
    "CUDA is a software toolkit that supports programming on Nvidia's GPUs. Luckily, we don't have to program in CUDA ourselves thanks to PyTorch. But the forerunners of deep learning must have written CUDA code themselves in C++ if they were to use GPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <span style=\"background-color:black\">nvidia-smi</span> terminal command is useful for checking the status of your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 27 10:49:49 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A3000 Laptop GPU     On | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   65C    P0               88W / 115W|   5128MiB /  6144MiB |     96%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1967      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A    131202      C   ...avid/venvs/Python310/bin/python3.10     5120MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To switch seamleslly from CPU to GPU or vice versa, you can define a <span style=\"background-color:black\">device</span> object that represents your current choice of device to run tensor operations on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for any tensor creation operations that accept a keyword argument <span style=\"background-color:black\">device</span>, you can pass this <span style=\"background-color:black\">device</span>  in and oyur new tensor will be right on that device. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_tensor = torch.ones((2,3), device=device)\n",
    "ones_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one tensor is already on a specific device, you can create a new tensor on the same device using <span style=\"background-color:black\">.*_like</span> methods. In this case, you dont need to pass in <span style=\"background-color:black\">device</span>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fives_tensor = torch.full_like(ones_tensor, 5.0)\n",
    "fives_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may often find yourself in a situation where you first create a tensor on CPU but later on want to move it to GPU. In that case, you can use the <span style=\"background-color:black\">.to</span> method of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2], [3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = x_data.to(device)\n",
    "x_data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Standar numpy-like indexing and slicing </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4,4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Joining tensors </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use <span style=\"background-color:black\">torch.cat</span> to concatenate a sequence of tensors along a given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1) # same as np.hstack\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.cat([tensor, tensor, tensor], dim=0) # same as np.vstack\n",
    "print(t2.shape)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 1., 1.],\n",
       "         [1., 0., 1., 1.],\n",
       "         [1., 0., 1., 1.],\n",
       "         [1., 0., 1., 1.]],\n",
       "\n",
       "        [[1., 0., 1., 1.],\n",
       "         [1., 0., 1., 1.],\n",
       "         [1., 0., 1., 1.],\n",
       "         [1., 0., 1., 1.]],\n",
       "\n",
       "        [[1., 0., 1., 1.],\n",
       "         [1., 0., 1., 1.],\n",
       "         [1., 0., 1., 1.],\n",
       "         [1., 0., 1., 1.]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.stack([tensor, tensor, tensor], dim=0) # same as np.stack\n",
    "print(t3.shape)\n",
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Arithmetic Operations </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1: \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "y2: \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "y3: \n",
      " tensor([[0.4798, 0.1759, 0.8198, 0.9446],\n",
      "        [0.7742, 0.6961, 0.5536, 0.0042],\n",
      "        [0.0060, 0.1001, 0.1396, 0.5689],\n",
      "        [0.5408, 0.9642, 0.7642, 0.9462]]) \n",
      "\n",
      "matmul_1: \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "y3: \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "z1: \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "z2: \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "matmul_2: \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two sensors. y1, y2, y3 will have the same value\n",
    "y1 = tensor @ tensor.T \n",
    "print(f\"y1: \\n {y1} \\n\")\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "print(f\"y2: \\n {y2} \\n\")\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "print(f\"y3: \\n {y3} \\n\")\n",
    "\n",
    "matmul_1 = torch.matmul(tensor, tensor.T, out=y3)\n",
    "print(f\"matmul_1: \\n {matmul_1} \\n\")\n",
    "print(f\"y3: \\n {y3} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "print(f\"z1: \\n {z1} \\n\")\n",
    "\n",
    "z2 = tensor.mul(tensor)\n",
    "print(f\"z2: \\n {z2} \\n\")\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "matmul_2 = torch.mul(tensor, tensor, out=z3)\n",
    "print(f\"matmul_2: \\n {matmul_2} \\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Single-element tensors</b> If you have a one-element tensor, for example by aggregating all values of a tensor into one vlaue, you can convert it to a Python numerical value using <span style=\"background-color:black\">item()</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In-place operations</b> Operations that store the result into the operand are called in-place. They are denoted by a <span style=\"background-color:black\">__</span> suffix. For example: <span style=\"background-color:black\">x.copy_(y)</span> , <span style=\"background-color:black\">x.t_()</span> , will change to <span style=\"background-color:black\">x</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. <b> Hence, their use is discouraged by the community. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, a tensor operation can be invoked in three ways, especially for the arithmetic operations.\n",
    "\n",
    "Take the <span style=\"background-color:black\">sqrt</span> operation as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1707, 0.5972, 0.7887],\n",
       "        [0.6107, 0.9641, 0.8558]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_tensor = torch.rand(2,3)\n",
    "\n",
    "sqrt_first = torch.sqrt(rand_tensor) # torch.[op](tensor, ...)\n",
    "sqrt_second = rand_tensor.sqrt()     # tensor.[op]()\n",
    "rand_tensor.sqrt_()                  # rensor.[op]_(...) (in-place)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compares two tensors if they are equal or not. Or all close\n",
    "torch.allclose(sqrt_first, sqrt_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(sqrt_first, rand_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy array from Tensor and to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t:{t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A change in the tensor reflects in the NumPy array. Because they share the same memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t:{t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A change in the NumPy array reflects in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t:{t}\")\n",
    "print(f\"n: {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
